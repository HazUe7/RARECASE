{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a28194f-323c-4026-8728-535e57ba8b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.1 Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, f1_score,\n",
    "    mean_squared_error, log_loss\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39ccbfe1-4c74-4fb3-a262-37ae7eb5f757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.2 Set path variables\n",
    "input_dir = 'LLM_extracted_IPD'\n",
    "output_dir = 'accuracy_metrics'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08794083-b8ab-4b97-9f04-fb1c572f0b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.3 Load the LLM output and human annotation data\n",
    "pilot_path = os.path.join(input_dir, \"pilot_output.csv\")\n",
    "df_llm = pd.read_csv(pilot_path)\n",
    "\n",
    "xls_path = 'NMDARE SR for STATISTICS 2021.01.26 1551 pts (excluding 16 Ab-neg and 83 post-infect) FINAL BEFORE CLEANING.xls'\n",
    "df_human = pd.read_excel(xls_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e5fb2ff-fd97-4be7-9708-7537ccb9788f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.4 Define a field mapping dictionary (llm : human)\n",
    "field_mapping = {\n",
    "    \"Age (years)\": \"Age at disease onset (years)\\n\\nblank=Not available\",\n",
    "    \"Sex\": \"Gender\\n\\n0=Male\\n1=Female\\nblank=Not available\",\n",
    "    \"Tumor\": \"Tumor\\n\\n0=No\\n1=Yes\\nblank=Not available\",\n",
    "    \"Number of Main Group of Symptoms\": \"Number of main group of symptoms\",\n",
    "    \"Length of Hospital Stay (days)\": \"Lenght of hospital stay (days) \\n\\nblank=Not available\",\n",
    "    \"T2 and FLAIR Hyperintensities\": \"T2 and FLAIR hyperintensities\"\n",
    "}\n",
    "\n",
    "# 0.5 Categorize the features by data type\n",
    "binary_features = [\"Sex\", \"Tumor\"]\n",
    "continuous_features = [\"Age (years)\", \"Number of Main Group of Symptoms\", \"Length of Hospital Stay (days)\"]\n",
    "categorical_features = [\"T2 and FLAIR Hyperintensities\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81e353f7-7f0b-4645-8558-0a6d64dddd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Define a function to match IPD by features between LLM-generated data and human-annotated data\n",
    "def create_matches(df_llm, df_human):\n",
    "    \"\"\"\n",
    "    Create a list of matched patient records\n",
    "    Args:\n",
    "        df_llm (pd.DataFrame): DataFrame containing LLM-generated data.\n",
    "        df_human (pd.DataFrame): DataFrame containing human-annotated data.\n",
    "    Returns:\n",
    "        matches (list of dict): Each dict contains:\n",
    "            - 'study': study identifier\n",
    "            - 'patient_idx': index of the patient record in df_llm\n",
    "            - 'y_true': dict of human-annotated feature values\n",
    "            - 'y_pred': dict of LLM-predicted feature values\n",
    "    \"\"\"\n",
    "    matches = []\n",
    "\n",
    "    for idx, row_llm in df_llm.iterrows():\n",
    "        y_true, y_pred = {}, {}\n",
    "        \n",
    "        study = row_llm[\"Study\"]\n",
    "        row_human = df_human[df_human[\"First author - Last author, year\"] == study].iloc[0]\n",
    "\n",
    "        for llm_field, human_field in field_mapping.items():                         ### For each feature, map values from LLM and human data\n",
    "            val_llm = row_llm.get(llm_field)\n",
    "            val_human = row_human.get(human_field)\n",
    "\n",
    "            y_pred[llm_field] = val_llm\n",
    "            y_true[llm_field] = val_human\n",
    "            \n",
    "        matches.append({\n",
    "            \"study\": study,\n",
    "            \"patient_idx\": idx,\n",
    "            \"y_true\": y_true,\n",
    "            \"y_pred\": y_pred\n",
    "        })\n",
    "        \n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d5e2dd2-e968-4dc6-8d73-62861c3f0a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 Define a function to evaluate the agreement on missing for a specific feature\n",
    "def evaluate_missing_value_agreement(matches, feature):\n",
    "    \"\"\"\n",
    "    Evaluate the agreement between predicted and true values on missing data.\n",
    "    Args:\n",
    "        matches (list of dict): A list of matched records, each containing:\n",
    "                                - \"y_true\": dict of human-annotated feature values\n",
    "                                - \"y_pred\": dict of model-generated feature values\n",
    "        feature (str): The name of the feature to evaluate missing value agreement on.\n",
    "    Returns:\n",
    "        missing_total (int): Total number of record-pairs where either true or predicted value is missing.\n",
    "        missing_match (int): Number of record-pairs where both true and predicted values are missing.\n",
    "        agreement_rate (float or None): Proportion of matching missing among all missing cases.\n",
    "    \"\"\"\n",
    "    missing_total, missing_match = 0, 0\n",
    "\n",
    "    for m in matches:\n",
    "        tv = m[\"y_true\"].get(feature)\n",
    "        pv = m[\"y_pred\"].get(feature)\n",
    "\n",
    "        tv_missing = tv is None or pd.isna(tv) or (isinstance(tv, str) and tv.strip() == \"\")          ### Define missingness conditions\n",
    "        pv_missing = pv is None or pd.isna(pv) or (isinstance(pv, str) and pv.strip() == \"\")\n",
    "\n",
    "        if tv_missing or pv_missing:\n",
    "            missing_total += 1\n",
    "            if tv_missing and pv_missing:\n",
    "                missing_match += 1\n",
    "\n",
    "    if missing_total > 0:\n",
    "        agreement_rate = missing_match / missing_total\n",
    "        print(f\" Missing Value Agreement for '{feature}': {missing_match} / {missing_total} matched ({agreement_rate:.1%})\")\n",
    "    else:\n",
    "        agreement_rate = None                                                                         ### None if no missing cases detected\n",
    "        print(f\" Missing Value Agreement for '{feature}': No missing cases detected.\")\n",
    "\n",
    "    return missing_total, missing_match, agreement_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "284e2ba4-bd70-4b7c-b5c6-3a185bc00028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3 Define a function for accuracy evaluation\n",
    "def evaluate_matrics(matches):\n",
    "    \"\"\"\n",
    "    Evaluate the accuracy of generated patient data by comparing it with human-annotated data.\n",
    "    Evaluation is performed separately for binary, continuous, and categorical features using appropriate metrics.\n",
    "    Args:\n",
    "        matches (list of dict): A list of matched records from model output and annotations.\n",
    "                                Each entry contains:\n",
    "                                    - \"study\": source study\n",
    "                                    - \"patient_idx\": patient index\n",
    "                                    - \"y_true\": human-annotated patient data (dict)\n",
    "                                    - \"y_pred\": model-generated patient data (dict)\n",
    "\n",
    "    Prints:\n",
    "        - Precision, Recall, F1 Score for binary features\n",
    "        - Mean Squared Error (MSE) for continuous features\n",
    "        - Cross-Entropy (Log Loss) for categorical features\n",
    "    \"\"\"    \n",
    "    # 1.3.1 Create lists to contain accuracy metrics\n",
    "    binary_results = []\n",
    "    continuous_results = []\n",
    "    categorical_results = []\n",
    "\n",
    "    # 1.3.2 Accuracy evaluation for binary features\n",
    "    for feature in binary_features:\n",
    "        y_true, y_pred = [], []\n",
    "\n",
    "        for m in matches:\n",
    "            tv = m[\"y_true\"].get(feature)\n",
    "            pv = m[\"y_pred\"].get(feature)\n",
    "\n",
    "            if tv is not None and pv is not None:               ### Only compare when both values are valid\n",
    "                y_true.append(tv)\n",
    "                y_pred.append(pv)\n",
    "\n",
    "        if not y_true:                                          ### Skip feature if no valid annotations are available\n",
    "            print(f\"\\n ⚠️ Skipping {feature} - lack of valid annotation.\")\n",
    "            continue\n",
    "\n",
    "        precision = precision_score(y_true, y_pred, average=\"binary\", zero_division=0)\n",
    "        recall = recall_score(y_true, y_pred, average=\"binary\", zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred, average=\"binary\", zero_division=0)\n",
    "\n",
    "        print(f\"\\n=== Binary Feature: {feature} ===\")\n",
    "        print(\"Precision:\", precision)\n",
    "        print(\"Recall:\", recall)\n",
    "        print(\"F1 Score:\", f1)\n",
    "        \n",
    "        missing_total, missing_match, agreement_rate = evaluate_missing_value_agreement(matches, feature)        ### Check agreement of missing for the processing feature\n",
    "        binary_results.append({\n",
    "            \"Feature\": feature,\n",
    "            \"Type\": \"Binary\",\n",
    "            \"Precision\": precision,\n",
    "            \"Recall\": recall,\n",
    "            \"F1\": f1,\n",
    "            \"Missing Total\": missing_total,\n",
    "            \"Missing Match\": missing_match,\n",
    "            \"Missing Match %\": agreement_rate\n",
    "        })\n",
    "\n",
    "    # 1.3.3 Accuracy evaluation for continuous features\n",
    "    for feature in continuous_features:\n",
    "        y_true, y_pred = [], []\n",
    "\n",
    "        for m in matches:\n",
    "            tv = m[\"y_true\"].get(feature)\n",
    "            pv = m[\"y_pred\"].get(feature)\n",
    "\n",
    "            if tv is not None and pv is not None and not pd.isna(tv) and not pd.isna(pv):  \n",
    "                y_true.append(tv)\n",
    "                y_pred.append(pv)                               ### Only compare when both values are valid\n",
    "\n",
    "        if not y_true:                                          ### Skip feature if no valid annotations are available\n",
    "            print(f\"\\n ⚠️ Skipping {feature} - lack of valid annotation.\")\n",
    "            continue\n",
    "\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        print(f\"\\n=== Continuous Feature: {feature} ===\")\n",
    "        print(\"MSE:\", mse)\n",
    "        \n",
    "        missing_total, missing_match, agreement_rate = evaluate_missing_value_agreement(matches, feature)\n",
    "        continuous_results.append({\n",
    "            \"Feature\": feature,\n",
    "            \"Type\": \"Continuous\",\n",
    "            \"MSE\": mse,\n",
    "            \"Missing Total\": missing_total,\n",
    "            \"Missing Match\": missing_match,\n",
    "            \"Missing Match %\": agreement_rate\n",
    "        })\n",
    "\n",
    "    # 1.3.4 Accuracy evaluation for text-based features\n",
    "    for feature in categorical_features:\n",
    "        y_true_raw, y_pred_raw = [], []\n",
    "\n",
    "        for m in matches:\n",
    "            tv = m[\"y_true\"].get(feature)\n",
    "            pv = m[\"y_pred\"].get(feature)\n",
    "\n",
    "            if tv and pv:                                       ### Filter out missing values and normalize text to lowercase\n",
    "                y_true_raw.append(str(tv).lower())\n",
    "                y_pred_raw.append(str(pv).lower())\n",
    "\n",
    "        if not y_true_raw:                                     \n",
    "            print(f\"\\n ⚠️ Skipping {feature} - lack of valid annotation.\")\n",
    "            continue                                            ### Skip feature if no valid annotations are available\n",
    "\n",
    "        if len(set(y_true_raw)) < 2 or len(set(y_pred_raw)) < 2 :   \n",
    "            print(f\"\\n ⚠️ Skipping log loss for {feature} — only one class found.\")\n",
    "            continue                                            ### Log loss requires at least 2 classes\n",
    "\n",
    "        all_classes = list(set(y_true_raw + y_pred_raw))\n",
    "        le = LabelEncoder()\n",
    "        le.fit(all_classes)                                     ### Encode text labels into numeric classes\n",
    "        y_true = le.transform(y_true_raw)\n",
    "        y_pred = le.transform(y_pred_raw)\n",
    "        \n",
    "        num_classes = len(le.classes_)\n",
    "        y_pred_prob = np.eye(num_classes)[y_pred]               ### One-hot encoding of predictions\n",
    "\n",
    "        ce_loss = log_loss(y_true, y_pred_prob, labels=range(num_classes))\n",
    "\n",
    "        print(f\"\\n=== Text Feature: {feature} ===\")\n",
    "        print(\"Classes:\", le.classes_)\n",
    "        print(\"Cross-Entropy (Log Loss):\", ce_loss)\n",
    "        \n",
    "        missing_total, missing_match, agreement_rate = evaluate_missing_value_agreement(matches, feature)\n",
    "        categorical_results.append({\n",
    "            \"Feature\": feature,\n",
    "            \"Type\": \"Categorical\",\n",
    "            \"LogLoss\": ce_loss,\n",
    "            \"Classes\": \", \".join(le.classes_),\n",
    "            \"Missing Total\": missing_total,\n",
    "            \"Missing Match\": missing_match,\n",
    "            \"Missing Match %\": agreement_rate\n",
    "        })\n",
    "        \n",
    "    return binary_results, continuous_results, categorical_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6405618e-59fb-4d25-aa32-f1e207d8f373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Binary Feature: Sex ===\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n",
      " Missing Value Agreement for 'Sex': No missing cases detected.\n",
      "\n",
      "=== Binary Feature: Tumor ===\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n",
      " Missing Value Agreement for 'Tumor': No missing cases detected.\n",
      "\n",
      "=== Continuous Feature: Age (years) ===\n",
      "MSE: 0.0\n",
      " Missing Value Agreement for 'Age (years)': No missing cases detected.\n",
      "\n",
      "=== Continuous Feature: Number of Main Group of Symptoms ===\n",
      "MSE: 4.8\n",
      " Missing Value Agreement for 'Number of Main Group of Symptoms': No missing cases detected.\n",
      "\n",
      "=== Continuous Feature: Length of Hospital Stay (days) ===\n",
      "MSE: 0.0\n",
      " Missing Value Agreement for 'Length of Hospital Stay (days)': 3 / 4 matched (75.0%)\n",
      "\n",
      "=== Text Feature: T2 and FLAIR Hyperintensities ===\n",
      "Classes: ['mri n.a./not done' 'normal' 'y']\n",
      "Cross-Entropy (Log Loss): 14.41746135564686\n",
      " Missing Value Agreement for 'T2 and FLAIR Hyperintensities': No missing cases detected.\n"
     ]
    }
   ],
   "source": [
    "# 2.1 Conduct accuracy evaluation on pilot set and save the results\n",
    "binary_metrics, continuous_metrics, categorical_metrics = evaluate_matrics(create_matches(df_llm, df_human))\n",
    "\n",
    "pd.DataFrame(binary_metrics).to_csv(os.path.join(output_dir, \"binary_metrics.csv\"), index=False)\n",
    "pd.DataFrame(continuous_metrics).to_csv(os.path.join(output_dir, \"continuous_metrics.csv\"), index=False)\n",
    "pd.DataFrame(categorical_metrics).to_csv(os.path.join(output_dir, \"categorical_metrics.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_env)",
   "language": "python",
   "name": "py38env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
